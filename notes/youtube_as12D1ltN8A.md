# YouTube 影片摘要

- **影片連結**: https://youtu.be/as12D1ltN8A
- **處理時間**: 2026-02-08 18:16
- **內容來源**: Whisper 轉錄
- **分析模型**: ollama

## AI 分析結果

## 影片分析：本地端模型 GPTOS120B 搭建与踩坑记录

**影片主要主题：**

影片记录了作者在尝试在本地端成功运行 GPTOS120B 模型，并结合 OpenCloud 工具的经历。分享了在尝试过程中遇到的各种技术难题、解决方案以及最终的搭建结果，并讨论了本地模型与云端模型的优缺点。

**影片摘要：**

1. **背景：** 受到观众建议启发，作者决定重新挑战本地端模型搭建，试图解决之前使用 OpenCloud 搭配本地模型“彩坑”时遇到的问题。

2. **挑战与踩坑：**
    * **Ollama:** 尝试增大 Context Window 到 64K 失败，导致推理速度极慢，因为 VRM 不足导致部分计算任务转移到 CPU。
    * **VLLM:**  尝试使用 VLLM 快速推理引擎，但由于下载模型文件失败而无法使用。
    * **LM Studio:** 使用 LM Studio 导致系统 RAM 占用过高，甚至接近当机。
    * **Llama Cicepi:**  在使用 Llama Cicepi 时，模型出现鬼打墙问题，经过调整发现问题在于 Chat Template 未自动读取以及 RAM 占用问题。

3. **最终解决方案：**
    * 通过使用 Llama Cicepi 配合 Nomeapp，成功将 GPTOS120B 模型完整运行在 GPU 上，并配合 OpenCloud 进行工具调用。
    * 强调了 Chat Template 的自动读取和 Nomeapp 的使用对于避免 RAM 占用问题的重要性。
    * 解决了 OpenCore 会保留对话历史的问题，建议定期清除 Telegram 中的对话记录。

4. **本地模型与云端模型的比较：**
    * 本地模型在执行任务时耗时更长，并且需要手动纠正错误，与云端模型相比性能差距明显。
    * 强调了本地模型的优势：资料隐私、对模型和数据处理方式的掌控以及技术探索的乐趣。

5. **总结与建议：**
    *  作者将详细的指令、配置、问题和解决方案整理到影片说明中。
    *  鼓励有兴趣的观众尝试，并欢迎经验丰富的观众提供指正。
    *  提醒如果只是需要一个易用的 AI 助手，云端方案更为省事。

**影片关键点：**

*   详细的技术细节分享，涉及推理引擎（Ollama, VLLM, LM Studio, Llama Cicepi）以及参数调整。
*   强调了特定参数（Context Window, Chat Template, Nomeapp）对模型运行的重要性。
*   探讨了本地模型和云端模型之间的权衡取舍。
*   体现了技术探索过程中的挫折与成功。

**Markdown 格式总结：**

**影片主题：** 本地端模型 GPTOS120B 搭建与踩坑记录

**核心内容：**

*   作者分享了尝试在本地端运行 GPTOS120B 的过程，涵盖了技术挑战与解决方案。
*   详细描述了在 Ollama, VLLM, LM Studio 和 Llama Cicepi 等推理引擎上的踩坑经历。
*   最终使用 Llama Cicepi + Nomeapp 成功搭建，并结合 OpenCloud 工具。
*   对比了本地模型与云端模型的优缺点，强调技术探索的价值。

**关键技术点：**

*   Context Window 的调整与 VRM 限制
*   Chat Template 的自动读取
*   Nomeapp 的使用
*   OpenCore 的对话历史处理


## 轉錄內容時間戳

- [00:00] 哈喽大家好,欢迎回到频道。
- [00:02] 还记得上次我分享了用OpenCloud搭配本地模型彩坑的经验,
- [00:06] 最后决定回到云端模型吗?
- [00:08] 上次影片发布后在留言区收到了几位先进的宝贵建议,
- [00:13] 特别要感谢Maxwell866提出可以试验OlamaLama.sepcp VLLM交叉测试的结果。
- [00:20] 原房内与Iverson558都提到调高context window。
- [00:24] 这些先进的建议我都有看,于是重新燃起了挑战决心。
- [00:28] 于是这次我又花了三四天的时间,尝试用不同的推理引擎,
- [00:33] 以及参数调整,想要找出一个真正可以在本地端稳定运行的解决方案。
- [00:38] 这次终于成功使用OpenCloud加的端模型GPTOS120B可以正常执行工具。
- [00:45] 虽然地端模型做出来的成果比云端大模型较差一点,
- [00:49] 但至少是成功的一步。
- [00:51] 在成功运行之前我做了哪些尝试,待会将1一讲解,
- [00:55] 包括,Ollama加大context window,到64K,使用VLAM烂入120B模型,
- [01:01] 使用LM Studio 120B模型,一开始只是很单纯地想直接在Ollama里调整settings。
- [01:07] 到64K,这样才能处理更长的对话历史。
- [01:11] 设定完成后,我测试了一下,发现一个很诡异的现象。
- [01:15] 虽然我的RTX Pro 6000有96GB的VRM,而且Ollama确实把模型载入到GPU的VRM里了。
- [01:23] 但推理速度超级慢,我用NVDSMine与工作管理员看了一下GPU使用率,
- [01:28] 发现竟然只有各位数,再看CPU使用率,确实很忙碌,这太奇怪了。
- [01:34] 明明模型在GPU VRM上,为什么会用CPU跑?
- [01:38] 后来我发现,当context window设定太大时,Ollama会因为VRM不足KVcash太大,
- [01:44] 自动offload一部分运算到CPU,即使我有96GB VRM,
- [01:49] 64K的context配上120B模型,在GitHub上Ollama的issue里也有人反映这个问题。
- [01:57] 结果就是,模型在GPU,但计算在CPU速度慢的要死,
- [02:02] 生成一个回答要等好几分钟,完全不能用。
- [02:05] 我尝试调整了NOMGPU参数,想要强制全部在GPU上跑。
- [02:13] 但还是一样的问题,Ollama在处理大context时的策略就是这样,
- [02:17] 没办法强制它全部用GPU,这个坑又让我浪费了几个小时。
- [02:22] 我决定再换一个方案试试看。
- [02:24] 速文VLLM是一个高效能的推理引擎,比Ollama快很多。
- [02:29] 支援Page Detention, Continuous Batching等进阶功能,听起来很厉害对吧。
- [02:34] 我想要载入GPT OS XC20B这个模型,但是问题来了,下载不完。
- [02:40] 我在HuggingFace上找到了这个模型,开始下载GPUF档案,
- [02:45] 这个模型分成两个档案,每个档案大概30多GB,加起来60GGB。
- [02:50] 我是使用Windows的Docadestop,不知道是我指令有问题还是什么缘故,
- [02:55] 会看到网路一直在下载模型,但都下载不完,
- [02:59] 欢迎有先进指证这个指令的错误。
- [03:06] 接下来我想说用Alma Studio看看,因为跟Ollama一样很亲民使用,
- [03:11] 借着下载GPT OS R12B模型,然后设定Contest Lens为64K,跟一些参数
- [03:18] 不知道为什么电脑的RAM不是VRAM直接被吃到99%,
- [03:22] 更糟的是模型载入的进度条卡在90%之内,左右就几乎不动了,等了
- [03:28] 十几分钟还是一样,我不确定是不是我之前调整了什么参数,
- [03:33] 但Reset参数也一样的状况,我只好放弃Alma Studio,后来想到Alma Studio
- [03:39] 底层是用Lama Deceptive,而且Maxwell大大也有提到Lama Deceptive,
- [03:44] 那我就来试试看,首先我从Github下载了Lama Deceptive的Windows
- [03:48] 版本,下载了戴库的支援的版本,然后我尝试启动Sever,这样下指令。
- [03:58] 但是新的问题又来了,跟Alma Studio一样会把RAM吃满,而且
- [04:03] 我用OpenCore测试了一下,发现模型会不断重复同样的句子,
- [04:08] 或是输出一堆无意义的文字,全部都是这种鬼打墙的内容。
- [04:12] 我找Cloud Code的研究,经过测试,发现关键是不要手动指定
- [04:17] chat template,让模型引擎自动从模型的GPUF答案内部读取Medadata中的
- [04:23] chat template,以及加上NoMap来避免RAM被吃满,然后改用参数重新启动Lama Deceptive。
- [04:30] 这次终于成功了,模型载入速度很快,回答也正常了,
- [04:35] 让我来解释一下每个参数的意义。
- [04:46] Lama,起CpServer,启动成功后,接下来要设定OpenCore连接到这个本地Server,
- [04:53] 首先,设定Lama CpProvider。这里有几个重点,BaseUR指向本地的Lama
- [04:59] GitSebure Server,然后ContextWindow设定为64K,对应前面Lama
- [05:05] GitSebure的设定,接着MaxToken设定为818的92,这是单次回应的最大
- [05:12] 长度,不是总Context,然后设定预设模型。这里还有一个重要的设定,
- [05:19] 移除Fallback模型。如果不移除这些Fallback,当本地模型出现任何问题时,
- [05:24] OpenCore可能会自动切换到云端模型,结果你以为在用本地模型,
- [05:30] 其实是在用云端模型,移除Fallback的方法是使用指令,把Fallbacks改成空阵列。
- [05:36] 这次就来看一下,我用OpenCore实际跟LL无铺动的demo,
- [05:40] 我希望它帮我规划冲绳自由行,而且要截图Googlemap的路线图,
- [05:46] 然后存到warden里面。这边有快转,可以看到这个简单的任务花费快,
- [05:51] 9分钟的时间,最后耗掉71000多个token,如果是给Core去做的话,一次就完成了。
- [05:58] 不像影片中我一直在纠正它的错误,
- [06:00] 一会没放图到ward,要不然还用全英文写到warden。
- [06:04] 还有一个重要的设定是,Talagram的历史讯息限制。我发现,
- [06:09] 即使在Talagram删除了对话历史,OpenCore还是会把之前的对话传给LL,
- [06:14] 这是因为OpenCore有自己的记忆体系统,不是从Talagram读取的,
- [06:19] 所以要记得在Talagram里面下MNU或Reset清除历史对话内容。这样OpenCore就不会把之前的对话传给LL,
- [06:27] 避免模型被旧对话干扰,后续的回答才会正常。
- [06:31] 好,那今天的番外片就到这边,回布一下。
- [06:34] 这次因为上一集留言区几位先进的建议,
- [06:37] 我又花了三四天的时间,测试了Ollama, VLMM Studio,
- [06:42] Lama, Cicepi,四种不同的推理引擎,踩了一堆坑。
- [06:47] Ollama,加大ContextWindow,结果变成用CPU在跑,慢到不行。
- [06:52] VLMM用Docker跑,模型一直下载不完。LMMM Studio,
- [06:56] 直接把系统RAM吃到99%,整台电脑快当机。最后用Lama,
- [07:02] Cicepi,一开始参数设错,模型鬼打墙一直重复输出,后来才发现是
- [07:07] ChatTemplate和Nomeapp的问题。最终成功的组合是Lama,Cicepi,
- [07:12] 搭配Nomeapp,让GPTOS120B完整跑在GPU上,再接上OpenCloud来做工具呼叫。
- [07:19] 不过老实说,从demo就可以看到,地端模型跟云端模型的差距还是蛮明显的。
- [07:25] 一个简单的任务花了快9分钟,中间还要一直纠正它的错误。
- [07:29] 如果是用Cloud的话可能一次就搞定了。
- [07:32] 所以这个方案目前不见得适合所有人。
- [07:35] 如果你只是想要一个好用的AI助理,不想折腾的话,
- [07:39] 用云端的方案还是比较省事。
- [07:41] 但如果你跟我一样,对技术有兴趣,
- [07:44] 想了解这些东西背后是怎么运作的,或是在意资料隐私,
- [07:48] 想要完全掌控自己的AI系统,那我觉得挑战本地模型还是蛮值得的。
- [07:53] 至少你会知道它用了什么模型,怎么处理你的资料,每个参数是干嘛的。
- [07:58] 我把这几天测试的指令、设定、预到的问题,跟解决方式都整理在影片说明栏的连结里了。
- [08:05] 想尝试的可以参考看看。
- [08:07] 设定过程中,如果遇到问题,欢迎在下面留言讨论。
- [08:11] 也希望有经验的先进们可以一起帮忙指正。
- [08:14] 如果觉得这个影片对你有帮助,帮我按个赞、订阅频道、开启小铃铛,
- [08:19] 你的支持是我继续分享的动力。那我们下次见,拜拜!
